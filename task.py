# -*- coding: utf-8 -*-
"""Task.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qNHDwNV_oB2alN2PbsA9vaZcHWAx5H9g
"""

!pip install requests_html
import pandas as pd
from requests_html import HTMLSession
from bs4 import BeautifulSoup
from time import sleep
from random import randint

"""## Part 1"""

s = HTMLSession()
url = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1'
headers = {'User-Agent': "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Safari/537.36 Edge/12.246"}

from IPython.core.interactiveshell import softspace
def getdata(url):
  r = s.get(url, headers=headers,proxies=proxies)
  # r.html.render(sleep=1)
  if r.status_code > 500:
        if "To discuss automated access to Amazon data please contact" in r.text:
            print("Page %s was blocked by Amazon. Please try using better proxies\n"%url)
        else:
            print("Page %s must have been blocked by Amazon as the status code was %d"%(url,r.status_code))
        return None
  soup = BeautifulSoup(r.html.html, 'html.parser')
  return soup

def getNextPage(soup):
  page = soup.find('span', {'class':'s-pagination-strip'})
  if not page.find('a', {'class':'s-pagination-item s-pagination-next s-pagination-disabled '}):
    link = page.find('a', {'class':'s-pagination-item s-pagination-next s-pagination-button s-pagination-separator'})
    if not link:
      return 
    url = 'http://www.amazon.in' + str(link['href'])
    return url
  else:
    return

pageUrls = []
while True:
  pageUrls.append(url)
  soup = getdata(url)
  url = getNextPage(soup)
  if not url:
    break

pageUrls

data = []

def appendInfo(content):
  for item in content:
    link = item.find('a', {'class':'a-link-normal s-no-outline'})
    url = 'https://www.amazon.in' + str(link['href'])
    name = item.find('span', {'class':'a-size-medium a-color-base a-text-normal'})
    price = item.find('span', {'class':'a-offscreen'})
    rating = item.find('span', {'class':'a-icon-alt'})
    review = item.find('span', {'class':'a-size-base s-underline-text'})
    if not rating:
      rating = "-"
      review = "-"
    else:
      rating = rating.text
      review = review.text
      review = str(review)
      review = review.replace("(", "")
      review = review.replace(")", "")
    data.append({
        'Product URL': url,
        'Product Name': name.text,
        'Product Price': price.text,
        'Rating': rating,
        'Reviews': review
    })

for page in pageUrls:
  soup = getdata(page)
  content = soup.find_all('div', {'class':'sg-col-20-of-24 s-result-item s-asin sg-col-0-of-12 sg-col-16-of-20 sg-col s-widget-spacing-small sg-col-12-of-16'})
  appendInfo(content)

len(data)

data = data[0:200]

data = pd.DataFrame(data)

data

"""## Part 2"""

import requests

# use to parse html text
from lxml.html import fromstring
from itertools import cycle
import traceback


def to_get_proxies():
	# website to get free proxies
	url = 'https://free-proxy-list.net/'

	response = requests.get(url)

	parser = fromstring(response.text)
	# using a set to avoid duplicate IP entries.
	proxies = set()

	for i in parser.xpath('//tbody/tr')[:10]:

		# to check if the corresponding IP is of type HTTPS
		if i.xpath('.//td[7][contains(text(),"yes")]'):

			# Grabbing IP and corresponding PORT
			proxy = ":".join([i.xpath('.//td[1]/text()')[0],
							i.xpath('.//td[2]/text()')[0]])

			proxies.add(proxy)
		return proxies

proxies = to_get_proxies()

# to rotate through the list of IPs
proxyPool = cycle(proxies)

# insert the url of the website you want to scrape.
url = ''

for i in range(1, 11):

	# Get a proxy from the pool
	proxy = next(proxyPool)
	print("Request #%d" % i)

	try:
		response = requests.get(url, proxies={"http": proxy, "https": proxy})
		print(response.json())

	except:
	
		# One has to try the entire process as most
		# free proxies will get connection errors
		# We will just skip retries.
  	 print("Skipping. Connection error")

links = data["Product URL"]

descriptions = []
asins = []
manufacturers = []
prodDescs = []

def clean(item):
  item = item.replace("  ", "")
  item = item.replace("\n", "")
  item = item.replace("\u200e", "")
  return item

def findASIN(soup):
  content = soup.find('div', {'class':'a-column a-span6 a-span-last'})
  if not content:
    asin = "-"
  else: 
    head = soup.find_all('th', {'class':'a-color-secondary a-size-base prodDetSectionEntry'})
    value = soup.find_all('td', {'class':'a-size-base prodDetAttrValue'})
    for i in range(len(head)):
      if head[i].text == " ASIN ":
        asin = value[i].text
        asin = clean(asin)
  asins.append(asin)

def findManufacturer(soup):
  content = soup.find('div', {'class':'a-column a-span6 a-span-last'})
  manufacturer = "-"
  if not content:
    manufacturer = "-"
  else:
    head = soup.find_all('th', {'class':'a-color-secondary a-size-base prodDetSectionEntry'})
    value = soup.find_all('td', {'class':'a-size-base prodDetAttrValue'})
    for i in range(len(head)):
      if head[i].text == " Manufacturer ":
        manufacturer = value[i].text
        manufacturer = clean(manufacturer)
        break
  manufacturers.append(manufacturer)

def getAllInfo(soup):
  description = soup.find('ul', {'class':'a-unordered-list a-vertical a-spacing-mini'})
  if not description:
    description = "-"
  else:
    description = description.text
  descriptions.append(description)
  findASIN(soup)
  findManufacturer(soup)
  content = soup.find('table', {'id':'productDetails_techSpec_section_1'})
  if not content:
    desc = "-"
  else:
    prodDesc = content.find_all('td', {'class':'a-size-base prodDetAttrValue'})
    desc = ""
    for item in prodDesc:
      item = str(item.text)
      item = item.replace(" ", "")
      desc += item + " "
      desc = clean(desc)
  prodDescs.append(desc)

i = 1
for url in links:
  soup = getdata(url)
  sleep(randint(2,10))
  print("Product: ", i)
  i+= 1
  getAllInfo(soup)

data['Description'] = descriptions
data['ASIN'] = asins
data['Manufacturer'] = manufacturers
data['Product Description'] = prodDescs

data

data.to_csv("final_data.csv")